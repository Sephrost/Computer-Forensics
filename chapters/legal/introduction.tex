\chapter{Introduction}
Back in the days, it was really easy to analyze the communication between two people, because the
technology was easier and know, meaning that the technology is \textit{between us}. Nowadays the
technology is more \textit{about us}, for example we use AI for facial recognition. Another example
are the social network algorithms that track our activity online. This means that we use the
technology to be "analyzed" instead of using it only to communicate. Most likely, the technology
will be \textit{in us}, which not only means robotics, but also be more integrated with human
beings.\\
In those scenarios, computer forensics became more complex, having to take in account no only the
quality of data(ie: a voice call), but also the quantity, which makes mistakes more likely to happen
in some instances. Another aspect of computer forensics will be to discern what is human from what
is a machine(ie: deep fakes, generative ai), and in particular where the liability lies.
Furthermore, it is necessary to handle the legal aspect of different countries, which is quite
difficult because of natural language limitations. An ideal solution would be to translate laws in
code, which is basically impossible, so we can approximate this solution with \textbf{legal design}
This has still to deal with two issues, which are information overloading(too much information), and
the fact that laws are written by lawyers for lawyers, which makes them difficult to understand.\\
Another aspect to consider is the GDPR, and the still to come AI act, for example the article 22
prevents automatic systems to make decisions having legal weight without the control of a human. An
example of this is the Lex Machina framework, which was able to predict the decisions of the judges
via ML methodologies, which was banned because it could influence the decisions of people and
judges. After all, false positives in digital forensics \textbf{can change people lives}.
